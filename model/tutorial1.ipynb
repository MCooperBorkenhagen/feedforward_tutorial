{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "This tutorial walks through a script that trains a feedforward orthography-to-phonology model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "We always start by importing some libraries the script is dependent upon. Here, we need `json`, `random`, and `time`. Additionally, we need our user-defined modules `learner` and `examples`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import time\n",
    "# user-defined:\n",
    "from learner import setupLearner\n",
    "import examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "Any model you specify requires certain configuration specs. We typically call these \"hyperparameters\". We can talk more about why we call them that (as opposed to say \"parameters\") sometime later. For now, just understand that some of the ingredients to running the model are certain specifications (values of one kind or another) to make it run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We call the configuration file cfg, by convention\n",
    "with open('params.json', 'r') as f:\n",
    "    cfg = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training environment\n",
    "We always need inputs and outputs, and these patterns can be thought of as the _training environment_. Sometimes, we have multiple inputs and multiple outputs, depending on which aspects of perception and cognition we are interested in. For now, we just have orthography and phonology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../inputs/orth.csv\n",
      "../inputs/phon.csv\n"
     ]
    }
   ],
   "source": [
    "orthography = examples.load('../inputs/orth.csv', simplify=True)\n",
    "phonology = examples.load('../inputs/phon.csv', simplify=True)\n",
    "words = [x.strip() for x in open('../inputs/words.csv','r').readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the learner\n",
    "In order to train the model, you have to initiate the learner. This involves giving the learner object (however you've defined it) the hyperparameters it needs to run. Here those values have been minimized to the hidden layer size, the maximum number of iterations, the input patterns, the output patterns, and those inputs and outputs we'd like to test at the end of training (in this case, all of them; test set = training set). We also save the start time to a variable so that at the end of training we can calculate the total training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "T = []\n",
    "\n",
    "start_time = time.time()\n",
    "training_environment = random.sample(range(len(orthography)), cfg['training_size'])\n",
    "T.append(training_environment)\n",
    "learner, model = setupLearner(\n",
    "    hidden_layer_sizes=cfg['hidden_size'],\n",
    "    max_iter=cfg['max_iter'],\n",
    "    input_patterns=orthography,\n",
    "    target_patterns=phonology,\n",
    "    test_set = training_environment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Fitting\" the model\n",
    "Now that we've saved the learner (and the untrained model) to variables, we can fit the model to the environment we've specified. There are different ways to configure the learner and model objects, but we've constructed a relatively simple one here. It might be confusing as to why `learner` and `model` exist as different objects, but we'll explain that more later. Just know that when you fit the `learner` to the environment, the state of the model is saved in `model`. Typically the method used to train the learner is called `fit()`. This is a conventional method name, and we use it here as well. We also print the time elapsed at the end of training, just to keep tabs on how long it is taking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14 minutes elapsed since start of learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcb/miniconda3/envs/python3_env/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "learner.fit(training_environment)\n",
    "end_time = time.time()\n",
    "print(round((end_time-start_time)/60, 2), \"minutes elapsed since start of learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the test data\n",
    "After the model is set, we can test it and save the data. Here we've done the absolute simplest thing: test the learner on all the patterns at the end of training. We then save this to an output file, and label the file with the date and time that the model runs (`dateID`). The code for testing looks complicated, but it really is just splitting up the training items and identifying which are train and which are test items. In our case here the test items are the same as the train items. We'll do fancier things later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dateID = time.asctime().replace(' ', '_')\n",
    "\n",
    "with open(str('../outputs/' + dateID + '.csv'), 'w') as f:\n",
    "    for i,w in enumerate(words):\n",
    "        f.write(\"{word:s},\".format(word=w))\n",
    "        isTrainingItem = [i in T[j] for j in range(len(T))]\n",
    "        f.write(\"{vec:s}\\n\".format(vec = ','.join([str(int(tf)) for tf in isTrainingItem])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All done\n",
    "Now you've trained and tested your model. You can move on to do the important stuff: analyze what the model knows and think about why."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 ('python3_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7c87c89642609041192033b4f33bc42b9a0cfb7ce4d636b18abcb06a9a62660e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
